{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a text classification model using noisy regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.models import candidate_subclass, Context, Candidate, StableLabel\n",
    "from snorkel.contrib.models.text import RawText\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.annotations import save_marginals\n",
    "from snorkel.learning.tensorflow import TextRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  720M    0  720M    0     0  7637k      0 --:--:--  0:01:36 --:--:-- 5567k\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    !curl https://data.consumerfinance.gov/api/views/s6ew-h6mp/rows.csv?accessType=DOWNLOAD > ./data/complaints.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Data load, filtering out irrelevant fields and rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects two types of products, for example, credit reporting and mortgage. \n",
    "Returns dataframe with complaint, and label - 1 = credit reporting complaints, -1 = other (mortgage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bd407488b890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomplaints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'complaints.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomplaints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcomplaints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snorkel/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snorkel/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snorkel/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snorkel/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/snorkel/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "complaints = pd.read_csv(os.path.join(data_dir,'complaints.csv'))\n",
    "complaints.info()\n",
    "complaints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset data to include products with narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_w_narrative = complaints[complaints['Product'].notnull() & \n",
    "                                  complaints['Consumer complaint narrative'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Debt collection'\n",
      " 'Credit reporting, credit repair services, or other personal consumer reports'\n",
      " 'Student loan' 'Credit card or prepaid card' 'Mortgage'\n",
      " 'Money transfer, virtual currency, or money service'\n",
      " 'Payday loan, title loan, or personal loan' 'Checking or savings account'\n",
      " 'Vehicle loan or lease' 'Credit card' 'Bank account or service'\n",
      " 'Credit reporting' 'Consumer Loan' 'Prepaid card' 'Payday loan'\n",
      " 'Money transfers' 'Other financial service' 'Virtual currency']\n",
      "Credit reporting, credit repair services, or other personal consumer reports    94174\n",
      "Debt collection                                                                 87506\n",
      "Mortgage                                                                        53334\n",
      "Credit reporting                                                                31588\n",
      "Student loan                                                                    21990\n",
      "Credit card or prepaid card                                                     21787\n",
      "Credit card                                                                     18838\n",
      "Bank account or service                                                         14885\n",
      "Checking or savings account                                                     13132\n",
      "Consumer Loan                                                                    9474\n",
      "Vehicle loan or lease                                                            5833\n",
      "Money transfer, virtual currency, or money service                               5555\n",
      "Payday loan, title loan, or personal loan                                        4504\n",
      "Payday loan                                                                      1747\n",
      "Money transfers                                                                  1497\n",
      "Prepaid card                                                                     1450\n",
      "Other financial service                                                           292\n",
      "Virtual currency                                                                   16\n",
      "Name: Product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(products_w_narrative['Product'].unique())\n",
    "print(products_w_narrative['Product'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit reporting only\n",
    "credit_reporting = products_w_narrative[products_w_narrative['Product'] == 'Credit reporting, ' + \n",
    "                                 'credit repair services, or other personal consumer reports']\n",
    "# mortgage only\n",
    "mortgage = products_w_narrative[products_w_narrative['Product'] == 'Mortgage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine into one dataframe, create labels (1 for credit reporting, -1 for mortgage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_narrative = credit_reporting['Consumer complaint narrative'].values\n",
    "positive_labels = pd.Series(np.ones(credit_narrative.shape[0]))\n",
    "positive_df = pd.DataFrame({'complaint': credit_narrative, 'label': positive_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortgage_narrative = mortgage['Consumer complaint narrative'].values\n",
    "negative_labels = pd.Series(np.full(mortgage_narrative.shape[0], -1))\n",
    "negative_df = pd.DataFrame({'complaint': mortgage_narrative, 'label': negative_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147508, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df = pd.concat([positive_df, negative_df], ignore_index=True)\n",
    "complaints_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    94174\n",
       "-1.0    53334\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complaints_df = complaints_df.take(np.random.permutation(len(complaints_df))[:1000])\n",
    "# complaints_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, testval = train_test_split(complaints_df, test_size=0.2, random_state=123)\n",
    "dev, test = train_test_split(testval, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create noisy labeling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions that define words, phrases, actions for use in labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_WORDS = r'\\bcredit\\b|\\breport|\\baddress\\b|\\bcreditor\\b|\\bexperian\\b|\\btransunion\\b|\\bequifax\\b|\\bfcra\\b|\\bcollector\\b|\\bdebt\\b|\\bconsumer\\b'              \n",
    "POSITIVE_PHRASE = r\"\\bcredit (report|agency|reporting|bureau|agencies)\"\n",
    "NEGATIVE_WORDS = r'\\bloan\\b|\\bmortgage\\b|\\bhouse\\b|\\bhome\\b|\\bprepayment\\b|\\bappraise|\\bforeclos\\btax'\n",
    "POSITIVE_ACTIONS = r'\\bremov|\\bdispute\\b'\n",
    "NEGATIVE_ACTIONS = r'\\bpredator|\\bmodifi\\brefinanc'\n",
    "\n",
    "FRAUD = r'\\bfraudulent (account|charges)'\n",
    "MONEYXFER = r'\\bmoney transfer'\n",
    "CREDIT_REPORTING = r'\\b(identity has been compromise(|d)|data breaches|inquiries to businesses|mistakes appear in my report|reporting incorrectly|dispute(|d)|(t|T)ransunion|derogatory|experian|identity theft)'\n",
    "CREDIT_REPAIR = r'\\b(credit repair|hard inquiry|inquiries to businesses|mistakes appear in my report|reporting incorrectly)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_credit(complaint):\n",
    "    if re.search(POSITIVE_WORDS, str(complaint), re.IGNORECASE):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_credit_phrase(complaint):\n",
    "    if re.search(POSITIVE_PHRASE, str(complaint), re.IGNORECASE):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_credit_actions(complaint):\n",
    "    if re.search(POSITIVE_ACTIONS, str(complaint), re.IGNORECASE):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_mortgage(complaint):\n",
    "    if re.search(NEGATIVE_WORDS, str(complaint), re.IGNORECASE):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def lf_mortgage_actions(complaint):\n",
    "    if re.search(NEGATIVE_ACTIONS, str(complaint), re.IGNORECASE):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lf_fraud(complaint):\n",
    "    if (re.search(FRAUD, str(complaint), re.IGNORECASE)\n",
    "        and not re.search(MONEYXFER, str(complaint), re.IGNORECASE)):\n",
    "        return 1\n",
    "    elif (re.search(FRAUD, str(complaint), re.IGNORECASE) \n",
    "          and re.search(MONEYXFER, str(complaint), re.IGNORECASE)):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lf_reporting(complaint):\n",
    "    if (re.search(CREDIT_REPORTING, str(complaint), re.IGNORECASE) \n",
    "        and not re.search(MONEYXFER, str(complaint), re.IGNORECASE)):\n",
    "        return 1\n",
    "    elif (re.search(CREDIT_REPORTING, str(complaint), re.IGNORECASE) \n",
    "           and re.search(MONEYXFER, str(complaint), re.IGNORECASE)):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def lf_repair(complaint):\n",
    "    if (re.search(CREDIT_REPAIR, str(complaint), re.IGNORECASE) \n",
    "        and not re.search(MONEYXFER, str(complaint), re.IGNORECASE) \n",
    "        and not re.search(CREDIT_REPORTING, str(complaint), re.IGNORECASE)\n",
    "       ):\n",
    "        return 1\n",
    "    elif ((re.search(CREDIT_REPAIR, str(complaint), re.IGNORECASE) \n",
    "           and re.search(MONEYXFER, str(complaint), re.IGNORECASE)) \n",
    "          or \n",
    "          (re.search(CREDIT_REPAIR, str(complaint), re.IGNORECASE) \n",
    "           and re.search(CREDIT_REPORTING, str(complaint), re.IGNORECASE))\n",
    "         ):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Snorkel objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates\n",
    "\n",
    "Candidate objects in Snorkel represent objects to be classified. In this case we are interested in classifying whether a narrative is positive - credit related or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = SnorkelSession()\n",
    "\n",
    "values = list(complaints_df.label.unique())\n",
    "print(values)\n",
    "\n",
    "# snorkel candidate, value if none defaults to binary (true, false)\n",
    "Narrative = candidate_subclass('Narrative', ['narrative'], values=values)\n",
    "\n",
    "# Make sure DB is cleared\n",
    "session.query(Context).delete()\n",
    "session.query(Candidate).delete()\n",
    "session.query(StableLabel).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contexts\n",
    "\n",
    "All Candidate objects point to one or more Context objects represent the raw data that they are rooted in. In this case, our candidates will each point to a single Context object representing the raw text of the complaint.\n",
    "\n",
    "Once we have defined the Context for each Candidate, we can commit them to the database. Note that we also split into three sets while doing this:\n",
    "\n",
    "- Training set (split=0): The narratives for which we have noisy, conflicting labels from our labeling functions; we will resolve these conflicts using the GenerativeModel and then use them as training data for the RNN\n",
    "- Development set (split=1): We will pretend that we do not have any noisy, conflicting labels for this split of the data, and use these to test the RNN's performance on unseen data\n",
    "- Test set (split=2): We will pretend that we do not have any noisy, conflicting labels for this split of the data, and use these to test the RNN's performance on unseen data    \n",
    "\n",
    "Note: we also store the gold/ annotator labels available for future comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147508\n"
     ]
    }
   ],
   "source": [
    "train_index = train.index\n",
    "train.index.str = np.asarray(str(x) for x in train.index)\n",
    "train_complaints = train.values[:, 0]\n",
    "train_labels = train.values[:, 1]\n",
    "\n",
    "for element in zip(train_index, train_complaints, train_labels):\n",
    "    split = 0\n",
    "    raw_text = RawText(stable_id=element[0], name=element[0], text=str(element[1]))\n",
    "    narrative = Narrative(narrative=raw_text, split=split)\n",
    "    session.add(narrative)\n",
    "    context_stable_id = \"~~\".join([str(element[0]), str(element[0])])\n",
    "    session.add(StableLabel(context_stable_ids=context_stable_id, \n",
    "                            annotator_name='gold',\n",
    "                            split=0,\n",
    "                            value=element[2]))\n",
    "dev_index = dev.index\n",
    "dev.index.str = np.asarray(str(x) for x in dev.index)\n",
    "dev_complaints = dev.values[:, 0]\n",
    "dev_labels = dev.values[:, 1]\n",
    "\n",
    "for element in zip(dev_index, dev_complaints, dev_labels):\n",
    "    split = 1\n",
    "    raw_text = RawText(stable_id=element[0], name=element[0], text=str(element[1]))\n",
    "    narrative = Narrative(narrative=raw_text, split=split)\n",
    "    session.add(narrative)\n",
    "    context_stable_id = \"~~\".join([str(element[0]), str(element[0])])\n",
    "    session.add(StableLabel(context_stable_ids=context_stable_id, \n",
    "                            annotator_name='gold',\n",
    "                            split=1,\n",
    "                            value=element[2]))\n",
    "\n",
    "test_index = test.index\n",
    "test.index.str = np.asarray(str(x) for x in test.index)\n",
    "test_complaints = test.values[:, 0]\n",
    "test_labels = test.values[:, 1]\n",
    "\n",
    "for element in zip(test_index, test_complaints, test_labels):\n",
    "    split = 2\n",
    "    raw_text = RawText(stable_id=element[0], name=element[0], text=str(element[1]))\n",
    "    narrative = Narrative(narrative=raw_text, split=split)\n",
    "    session.add(narrative)\n",
    "    context_stable_id = \"~~\".join([str(element[0]), str(element[0])])\n",
    "    session.add(StableLabel(context_stable_ids=context_stable_id, \n",
    "                            annotator_name='gold',\n",
    "                            split=2,\n",
    "                            value=element[2]))\n",
    "\n",
    "session.commit()\n",
    "\n",
    "# number of datapoints\n",
    "print(session.query(Narrative).count())\n",
    "# load ground truth labels\n",
    "train_cand_labels = train_labels\n",
    "dev_cand_labels = dev_labels\n",
    "test_cand_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number labeled: 11028\n"
     ]
    }
   ],
   "source": [
    "# test one labeling function\n",
    "labeled = []\n",
    "for c in session.query(Narrative).filter(Narrative.split == 1).all():\n",
    "    if lf_credit(c) != 0:\n",
    "        labeled.append(c)\n",
    "print(\"Number labeled:\", len(labeled))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "\n",
    "Next, we assign labeling functions for each of the training candidates in a sparse matrix (which will also automatically be saved to the Snorkel database), with one row for each candidate and one column for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35/118006 [00:00<05:37, 349.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118006/118006 [05:12<00:00, 377.81it/s]\n"
     ]
    }
   ],
   "source": [
    "LFs = [lf_credit, lf_credit_phrase, lf_credit_actions, lf_mortgage, lf_mortgage_actions, lf_fraud, lf_reporting, lf_repair]\n",
    "# apply labeling functions\n",
    "labeler = LabelAnnotator(lfs = LFs)\n",
    "L_train = labeler.apply(split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narrative(Raw Text XXXX XXXX XXXX- XX-XX-2016 Experian XXXX XXXX XXXX -XX/XX/2016 XXXX XXXX XXXX-XX/XX/2016 XXXX XXXX XXXX-XX/XX/2016 The inquiries that I have written above are unauthorized. I did not allow anyone to use my personal information to obtain goods. Experian and XXXX have not removed the fraud inquiries from my credit report even though i have disputed them multiple times.)\n",
      "LabelKey (lf_credit)\n",
      "                     j  Coverage  Overlaps  Conflicts\n",
      "lf_credit            0  0.751606  0.674652   0.273452\n",
      "lf_credit_phrase     1  0.437503  0.437503   0.128214\n",
      "lf_credit_actions    2  0.328822  0.322585   0.099020\n",
      "lf_mortgage          3  0.456155  0.282037   0.277833\n",
      "lf_mortgage_actions  4  0.011440  0.011347   0.007144\n",
      "lf_fraud             5  0.020448  0.020143   0.003127\n",
      "lf_reporting         6  0.322128  0.319111   0.086097\n",
      "lf_repair            7  0.027456  0.026846   0.018194\n"
     ]
    }
   ],
   "source": [
    "# see what's going on\n",
    "print(L_train.get_candidate(session, 0))\n",
    "print(L_train.get_key(session, 0))\n",
    "print(L_train.lf_stats(session))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training generative model\n",
    "\n",
    "Check if it assumes that the LFs have no dependencies by default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n"
     ]
    }
   ],
   "source": [
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=20, decay=0.95, step_size=0.1/L_train.shape[0], reg_param=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the generative model\n",
    "\n",
    "#### Probabilistic Label Statistics\n",
    "We view the distribution of weak labels produced by our generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare learned accuracies vs empirical accuracies\n",
    "Learned accuracies from our generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.64624 recall: 0.96901 F-beta: 0.77537\n"
     ]
    }
   ],
   "source": [
    "accuracy = gen_model.score(L_train, train_cand_labels)\n",
    "accuracy[0]\n",
    "print(\"precision: {:.5f}\".format(accuracy[0]), \"recall: {:.5f}\".format(accuracy[1]), \n",
    "      \"F-beta: {:.5f}\".format(accuracy[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical accuracies of our labeling functions - majority vote. Also calculating precision, recall and f-beta for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8135772757317424\n",
      "Number incorrect:21999\n",
      "precision: 0.79070 recall: 0.96268 F-beta: 0.86826\n"
     ]
    }
   ],
   "source": [
    "# Collect the majority vote answer for each complaint\n",
    "mv = []\n",
    "for i in range(L_train.shape[0]):\n",
    "    if np.diff(L_train[i].indptr) != 0:   #indicates that there is no coverage for a particular datapoint\n",
    "        c = Counter([L_train[i,j] for j in L_train[i].nonzero()[1]])\n",
    "        #print(c)\n",
    "        mv.append(c.most_common(1)[0][0])\n",
    "        #print(c.most_common(1)[0][0])\n",
    "    else:\n",
    "        mv.append(-1) # assume that no label is equivalent to a negative example\n",
    "\n",
    "mv = np.array(mv)\n",
    "\n",
    "# Count the number correct by majority vote\n",
    "n_correct = np.sum([1 for i in range(L_train.shape[0]) if mv[i] == train_cand_labels[i]])\n",
    "print (\"Accuracy:{}\".format(n_correct / float(L_train.shape[0])))\n",
    "print (\"Number incorrect:{}\".format(L_train.shape[0] - n_correct))\n",
    "\n",
    "# Compute and return precision, recall, and F1 score\n",
    "tp = (0.5 * (mv * train_cand_labels + 1))[mv == 1].sum()\n",
    "pred_pos = mv[mv == 1].sum()\n",
    "p = tp / float(pred_pos) if pred_pos > 0 else 0.0\n",
    "pos = train_cand_labels[train_cand_labels == 1].sum()\n",
    "r = tp / float(pos) if pos > 0 else 0.0\n",
    "\n",
    "# Compute general F-beta score\n",
    "beta=1\n",
    "if p + r > 0:\n",
    "    f_beta = (1 + beta**2) * ((p * r) / (((beta**2) * p) + r))\n",
    "else:\n",
    "    f_beta = 0.0\n",
    "p, r, f_beta\n",
    "print(\"precision: {:.5f}\".format(p), \"recall: {:.5f}\".format(r), \"F-beta: {:.5f}\".format(f_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a more detailed score (true positives, false positives, true negatives, false negatives) on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 34/14751 [00:00<00:43, 334.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14751/14751 [00:37<00:00, 389.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.966\n",
      "Neg. class accuracy: 0.0648\n",
      "Precision            0.646\n",
      "Recall               0.966\n",
      "F1                   0.774\n",
      "----------------------------------------\n",
      "TP: 9094 | FP: 4992 | TN: 346 | FN: 319\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply(split=1)\n",
    "# score it\n",
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, dev_cand_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the predictions of the generative model on the train and test set back to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 118006 marginals\n"
     ]
    }
   ],
   "source": [
    "save_marginals(session, L_train, train_marginals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a ML model based on the probabilistic labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(Narrative).filter(Narrative.split == 0).order_by(Narrative.id).all()\n",
    "dev_cands   = session.query(Narrative).filter(Narrative.split == 1).order_by(Narrative.id).all()\n",
    "test_cands  = session.query(Narrative).filter(Narrative.split == 2).order_by(Narrative.id).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the number of training samples are lower than what's present in the training set, it excludes samples where there was no LF coverage (train_marginals.shape[0] - (train_marginals == 0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextRNN] Training model\n",
      "[TextRNN] n_train=113422  #epochs=100  batch size=10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[10,5729,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node embedding_lookup (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py:81) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node Mean (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py:77) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3220, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-b444b3385591>\", line 13, in <module>\n    lstm.train(X_train=train_cands, Y_train=train_marginals, X_dev=dev_cands, Y_dev=dev_cand_labels, **train_kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py\", line 173, in train\n    word_dict=self.word_dict, max_len=max_len, **kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py\", line 206, in train\n    self._build_new_graph_session(**kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py\", line 117, in _build_new_graph_session\n    self._build_model(**model_kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py\", line 81, in _build_model\n    inputs = tf.nn.embedding_lookup(embedding, self.sentences)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 316, in embedding_lookup\n    transform_fn=None)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 3273, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3748, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10,5729,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node embedding_lookup (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py:81) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node Mean (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py:77) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10,5729,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node embedding_lookup}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node Mean}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b444b3385591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNarrative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Note: Y_train are the marginals but Y_dev are the gold/ ground truth labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_marginals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_cands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_cand_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, X_dev, max_sentence_length, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Train model- note we pass word_dict through here so it gets saved...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         super(RNNBase, self).train(X_train, Y_train, X_dev=X_dev,\n\u001b[0;32m--> 173\u001b[0;31m             word_dict=self.word_dict, max_len=max_len, **kwargs)\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_marginals_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/snorkel/snorkel/learning/tensorflow/noise_aware_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train, n_epochs, lr, batch_size, rebalance, X_dev, Y_dev, print_freq, dev_ckpt, dev_ckpt_delay, save_dir, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;31m# Run training step and evaluate loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 epoch_loss, _ = self.session.run(\n\u001b[0;32m--> 239\u001b[0;31m                     [self.loss, self.optimizer], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[10,5729,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node embedding_lookup (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py:81) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node Mean (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py:77) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'embedding_lookup', defined at:\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 781, in inner\n    self.run()\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3220, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-b444b3385591>\", line 13, in <module>\n    lstm.train(X_train=train_cands, Y_train=train_marginals, X_dev=dev_cands, Y_dev=dev_cand_labels, **train_kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py\", line 173, in train\n    word_dict=self.word_dict, max_len=max_len, **kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py\", line 206, in train\n    self._build_new_graph_session(**kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py\", line 117, in _build_new_graph_session\n    self._build_model(**model_kwargs)\n  File \"/home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py\", line 81, in _build_model\n    inputs = tf.nn.embedding_lookup(embedding, self.sentences)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 316, in embedding_lookup\n    transform_fn=None)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 133, in _embedding_lookup_and_transform\n    result = _clip(array_ops.gather(params[0], ids, name=name),\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 3273, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3748, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/nisha/anaconda3/envs/snorkel/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10,5729,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node embedding_lookup (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/rnn/rnn_base.py:81) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[node Mean (defined at /home/nisha/snorkel/snorkel/learning/tensorflow/noise_aware_model.py:77) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = {\n",
    "    'lr':         0.01,\n",
    "    'dim':        100,\n",
    "    'n_epochs':   100,\n",
    "    'dropout':    0.2,\n",
    "    'print_freq': 5,\n",
    "    'seed': 123,\n",
    "    'batch_size': 10\n",
    "}\n",
    "\n",
    "lstm = TextRNN(seed=123, cardinality=Narrative.cardinality)\n",
    "# Note: Y_train are the marginals but Y_dev are the gold/ ground truth labels\n",
    "lstm.train(X_train=train_cands, Y_train=train_marginals, X_dev=dev_cands, Y_dev=dev_cand_labels, **train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dev = lstm.score(dev_cands, dev_cand_labels)\n",
    "print(\"precision: {:.5f}\".format(accuracy_dev[0]), \"recall: {:.5f}\".format(accuracy_dev[1]), \n",
    "      \"F-beta: {:.5f}\".format(accuracy_dev[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = lstm.score(test_cands, test_cand_labels)\n",
    "print(\"precision: {:.5f}\".format(accuracy_test[0]), \"recall: {:.5f}\".format(accuracy_test[1]), \n",
    "      \"F-beta: {:.5f}\".format(accuracy_test[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
